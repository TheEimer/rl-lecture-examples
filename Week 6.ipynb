{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, exploration_rate):\n",
    "    if np.random.uniform(0, 1) < exploration_rate:\n",
    "        return env.action_space.sample()\n",
    "    q_values = Q(torch.from_numpy(state).float()).detach().numpy()\n",
    "    return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vfa_update(states, actions, rewards, dones, next_states):\n",
    "    optimizer.zero_grad()\n",
    "    states = torch.from_numpy(np.array(states)).float()\n",
    "    actions = torch.from_numpy(np.array(actions)).unsqueeze(-1)\n",
    "    rewards = torch.from_numpy(np.array(rewards)).float()\n",
    "    dones = torch.from_numpy(np.array(dones)).float()\n",
    "    next_states = torch.from_numpy(np.array(next_states)).float()\n",
    "\n",
    "    \"\"\"\n",
    "    value function approximation update\n",
    "    \"\"\"\n",
    "    q_values = torch.gather(Q(states), dim=-1, index=actions).squeeze()\n",
    "    target_q_values = rewards + \\\n",
    "        (1 - dones) * DISCOUNT_FACTOR * Q(next_states).max(dim=-1)[0].detach()\n",
    "    loss = F.mse_loss(q_values, target_q_values)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(num_episodes, exploration_rate=0.1, use_per=False):\n",
    "    rewards = []\n",
    "    replay_buffer = deque(maxlen=int(1e5))\n",
    "    for episode in range(num_episodes):\n",
    "        rewards.append(0)\n",
    "        obs = env.reset()\n",
    "        state = obs\n",
    "\n",
    "        for t in range(MAX_EPISODE_LENGTH):\n",
    "            action = policy(state, exploration_rate)\n",
    "\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            next_state = obs\n",
    "            replay_buffer.append((state, action, reward, done, next_state))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            rewards[-1] += reward\n",
    "\n",
    "            if len(replay_buffer) >= BATCH_SIZE:\n",
    "                if use_per:\n",
    "                    # Prioritized Experience Replay\n",
    "                    weights = np.linspace(0, 100, len(replay_buffer))\n",
    "                    batch = random.choices(replay_buffer, k=BATCH_SIZE, weights=weights)\n",
    "                else:\n",
    "                    batch = random.choices(replay_buffer, k=BATCH_SIZE)\n",
    "                vfa_update(*zip(*batch))\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode % (num_episodes / 100) == 0:\n",
    "            print(\"Mean Reward: \", np.mean(rewards[-int(num_episodes / 100):]))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODE_LENGTH = 1000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "Q = nn.Sequential(nn.Linear(np.prod(env.observation_space.shape),\n",
    "                            64), nn.ReLU(), nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, env.action_space.n))\n",
    "\n",
    "optimizer = optim.Adam(Q.parameters(), lr=5e-4)\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward:  -515.4510551292258\n",
      "Mean Reward:  -282.57291779523786\n",
      "Mean Reward:  -119.60106094478347\n",
      "Mean Reward:  -140.1173111498822\n",
      "Mean Reward:  -40.988395046556946\n",
      "Mean Reward:  2.123535716905363\n",
      "Mean Reward:  -10.17904552999978\n",
      "Mean Reward:  -159.72790976154744\n",
      "Mean Reward:  -71.73557629748355\n",
      "Mean Reward:  -59.5558794142258\n",
      "Mean Reward:  -62.324745317563824\n",
      "Mean Reward:  -97.40087034218512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2239c21d6ed4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-96068eccdef1>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(num_episodes, exploration_rate)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;31m# Prioritized Experience Replay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0;31m#batch = random.choices(replay_buffer, k=BATCH_SIZE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mvfa_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/reinforcement-learning/lib/python3.8/random.py\u001b[0m in \u001b[0;36mchoices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.0\u001b[0m    \u001b[0;31m# convert to float for a small speed improvement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mcum_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_accumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot specify both weights and cumulative weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q_learning(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
